{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import networkx as nx\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "from leafer import Leafer\n",
    "import numpy as np\n",
    "import pickle\n",
    "import os\n",
    "import glob\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "from nltk.corpus import wordnet as wn\n",
    "\n",
    "\n",
    "seed = 42\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "G = nx.DiGraph()\n",
    "for synset in (wn.all_synsets('n')):\n",
    "    name = synset.name()\n",
    "    G.add_node(name)\n",
    "    hyponyms = synset.hyponyms()\n",
    "\n",
    "    for hypo in hyponyms:\n",
    "        new_name = hypo.name()\n",
    "        G.add_node(new_name)\n",
    "        G.add_edge(name, new_name)\n",
    "\n",
    "for synset in (wn.all_synsets('v')):\n",
    "    name = synset.name()\n",
    "    G.add_node(name)\n",
    "    hyponyms = synset.hyponyms()\n",
    "\n",
    "    for hypo in hyponyms:\n",
    "        new_name = hypo.name()\n",
    "        G.add_node(new_name)\n",
    "        G.add_edge(name, new_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "95882"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(G.nodes())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exclude test\n",
    "\n",
    "# from MAGS\n",
    "cs_test_path = '../../TaxonomyEnrichment/data/MAG_CS/test_nodes.pickle'\n",
    "psy_test_path = '../../TaxonomyEnrichment/data/psychology/test_nodes.pickle'\n",
    "noun_test_path = '../../TaxonomyEnrichment/data/noun/test_nodes.pickle'\n",
    "verb_test_path = '../../TaxonomyEnrichment/data/verb/test_nodes.pickle'\n",
    "\n",
    "with open(cs_test_path, 'rb') as f:\n",
    "    cs_test = pickle.load(f)\n",
    "\n",
    "with open(psy_test_path, 'rb') as f:\n",
    "    psy_test = pickle.load(f)\n",
    "\n",
    "with open(noun_test_path, 'rb') as f:\n",
    "    noun_test = pickle.load(f)\n",
    "\n",
    "with open(verb_test_path, 'rb') as f:\n",
    "    verb_test = pickle.load(f)\n",
    "\n",
    "k = 0\n",
    "for node, parents in cs_test:\n",
    "    for i in range(10):\n",
    "        true_name = f'{node}.n.0{i}'\n",
    "        if true_name in G.nodes():\n",
    "            G.remove_node(true_name)\n",
    "            k += 1\n",
    "    \n",
    "for node, parents in psy_test:\n",
    "    for i in range(10):\n",
    "        true_name = f'{node}.n.0{i}'\n",
    "        if true_name in G.nodes():\n",
    "            G.remove_node(true_name)\n",
    "            k += 1\n",
    "\n",
    "for node, parents in verb_test:\n",
    "    if node in G.nodes():\n",
    "        G.remove_node(node)\n",
    "        k += 1\n",
    "\n",
    "for node, parents in noun_test:\n",
    "    if node in G.nodes():\n",
    "        G.remove_node(node)\n",
    "        k += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2096"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from Hypernym Discovery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_path = '../../SemEval2018-Task9/custom_datasets/1A.english.pickle'\n",
    "medical_path = '../../SemEval2018-Task9/custom_datasets/2A.medical.pickle'\n",
    "music_path = '../../SemEval2018-Task9/custom_datasets/2B.music.pickle'\n",
    "\n",
    "with open(main_path, 'rb') as f:\n",
    "    main = pickle.load(f)\n",
    "\n",
    "with open(medical_path, 'rb') as f:\n",
    "    medical = pickle.load(f)\n",
    "\n",
    "with open(music_path, 'rb') as f:\n",
    "    music = pickle.load(f)\n",
    "\n",
    "\n",
    "for elem in main:\n",
    "    node = elem['children'].replace(' ', '_')\n",
    "    for i in range(10):\n",
    "        true_name = f'{node}.n.0{i}'\n",
    "        if true_name in G.nodes():\n",
    "            G.remove_node(true_name)\n",
    "            k += 1\n",
    "\n",
    "for elem in medical:\n",
    "    node = elem['children'].replace(' ', '_')\n",
    "    for i in range(10):\n",
    "        true_name = f'{node}.n.0{i}'\n",
    "        if true_name in G.nodes():\n",
    "            G.remove_node(true_name)\n",
    "            k += 1\n",
    "\n",
    "for elem in music:\n",
    "    node = elem['children'].replace(' ', '_')\n",
    "    for i in range(10):\n",
    "        true_name = f'{node}.n.0{i}'\n",
    "        if true_name in G.nodes():\n",
    "            G.remove_node(true_name)\n",
    "            k += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4257"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From TEXEVAL\n",
    "data = 'environment'\n",
    "env_path = \"../../TExEval-2_testdata_1.2/gs_taxo/EN/\" + str(data) + \"_eurovoc_en.taxo\"\n",
    "sci_path =  \"../../TExEval-2_testdata_1.2/gs_taxo/EN/\" + 'science' + \"_eurovoc_en.taxo\"\n",
    "G_test = nx.DiGraph()\n",
    "\n",
    "with open(env_path, \"r\") as f:\n",
    "    for line in f:\n",
    "        idx, hypo, hyper = line.split(\"\\t\")\n",
    "        hyper = hyper.replace(\"\\n\", \"\")\n",
    "        G_test.add_node(hypo)\n",
    "        G_test.add_node(hyper)\n",
    "        G_test.add_edge(hyper, hypo)\n",
    "\n",
    "for node in G_test.nodes():\n",
    "    for i in range(10):\n",
    "        true_name = f'{node}.n.0{i}'\n",
    "        if true_name in G.nodes():\n",
    "            G.remove_node(true_name)\n",
    "            k += 1\n",
    "\n",
    "with open(sci_path, \"r\") as f:\n",
    "    for line in f:\n",
    "        idx, hypo, hyper = line.split(\"\\t\")\n",
    "        hyper = hyper.replace(\"\\n\", \"\")\n",
    "        G_test.add_node(hypo)\n",
    "        G_test.add_node(hyper)\n",
    "        G_test.add_edge(hyper, hypo)\n",
    "\n",
    "for node in G_test.nodes():\n",
    "    for i in range(10):\n",
    "        true_name = f'{node}.n.0{i}'\n",
    "        if true_name in G.nodes():\n",
    "            G.remove_node(true_name)\n",
    "            k += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('restrain.v.01', 'inhibit.v.04'), ('inhibit.v.04', 'restrain.v.01')]\n"
     ]
    }
   ],
   "source": [
    "while True:\n",
    "    try:\n",
    "        cycle = nx.find_cycle(G)\n",
    "        print(cycle)\n",
    "        G.remove_edge(*cycle[0])\n",
    "    except:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predict_hypernym 36775 36775\n",
      "predict_hypernym 31 31\n"
     ]
    }
   ],
   "source": [
    "l = Leafer(G)\n",
    "\n",
    "\n",
    "train, test = l.split_train_test(\n",
    "    generation_depth=0,\n",
    "    p=0.001,\n",
    "    p_divide_leafs=0.5,\n",
    "    min_to_test_rate=0.5,\n",
    "    weights=[0.00, 0.0, 0.0, 0.00, 0.00, 1.],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_definitions(elem):\n",
    "    if elem['case'] == 'predict_hypernym':\n",
    "        elem['child_def'] = wn.synset(elem['children'].replace(' ', '_')).definition()\n",
    "      #  elem['parent_def'] = wn.synset(elem['parents']).definition()\n",
    "    elif elem['case'] == 'predict_multiple_hypernyms':\n",
    "        elem['child_def'] = wn.synset(elem['children'].replace(' ', '_')).definition()\n",
    "\n",
    "    elif elem['case'] == 'simple_triplet_grandparent':\n",
    "        elem['child_def'] = wn.synset(elem['children'].replace(' ', '_')).definition()\n",
    "        elem['grandparent_def'] = wn.synset(elem['grandparents']).definition()\n",
    "    elif elem['case'] == 'only_child_leaf':\n",
    "        elem['grandparent_def'] = wn.synset(elem['grandparents']).definition()\n",
    "        elem['parent_def'] = wn.synset(elem['parents']).definition()\n",
    "    elif elem['case'] == 'simple_triplet_2parent':\n",
    "        elem['1parent_def'] = wn.synset(elem['parents'][0]).definition()\n",
    "        elem['2parent_def'] = wn.synset(elem['parents'][1]).definition()\n",
    "    else:\n",
    "        elem['parent_def'] = wn.synset(elem['parents']).definition()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "for i, elem in enumerate(train):\n",
    "    try:\n",
    "        add_definitions(elem)\n",
    "    except:\n",
    "        print(i, elem)\n",
    "        train.remove(elem)\n",
    "\n",
    "counter = 0\n",
    "for i, elem in enumerate(test):\n",
    "    try:\n",
    "        add_definitions(elem)\n",
    "    except:\n",
    "        print(i, elem)\n",
    "        counter += 1\n",
    "        test.remove(elem)\n",
    "\n",
    "print(counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_out = '/home/LLM_Taxonomy/wnet/unified_clean_wnet_noun_verb_def_train.pickle'\n",
    "test_out = '/home/LLM_Taxonomy/wnet/unified_clean_wnet_noun_verb_def_test.pickle'\n",
    "\n",
    "with open(train_out, 'wb') as f:\n",
    "    pickle.dump(train, f)\n",
    "\n",
    "with open(test_out, 'wb') as f:\n",
    "    pickle.dump(test, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NON SELECTIVE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G = nx.DiGraph()\n",
    "for synset in (wn.all_synsets('n')):\n",
    "    name = synset.name()\n",
    "    G.add_node(name)\n",
    "    hyponyms = synset.hyponyms()\n",
    "\n",
    "    for hypo in hyponyms:\n",
    "        new_name = hypo.name()\n",
    "        G.add_node(new_name)\n",
    "        G.add_edge(name, new_name)\n",
    "\n",
    "for synset in (wn.all_synsets('v')):\n",
    "    name = synset.name()\n",
    "    G.add_node(name)\n",
    "    hyponyms = synset.hyponyms()\n",
    "\n",
    "    for hypo in hyponyms:\n",
    "        new_name = hypo.name()\n",
    "        G.add_node(new_name)\n",
    "        G.add_edge(name, new_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = Leafer(G)\n",
    "# iterator = l.leafs_generator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predict_hypernym 44772 44772\n",
      "predict_hypernym 49 49\n"
     ]
    }
   ],
   "source": [
    "train, test = l.split_train_test(\n",
    "    generation_depth=0,  \n",
    "    p=0.001, \n",
    "    p_divide_leafs=0.5,\n",
    "    min_to_test_rate=0.5,\n",
    "    weights=[0.00, 0.0, 0.0, 0.00, 0.00, 1.], \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([{'children': 'window_seat.n.01',\n",
       "   'parents': 'bench.n.01',\n",
       "   'grandparents': None,\n",
       "   'case': 'predict_hypernym'},\n",
       "  {'children': 'pummel.v.01',\n",
       "   'parents': 'hit.v.03',\n",
       "   'grandparents': None,\n",
       "   'case': 'predict_hypernym'},\n",
       "  {'children': 'spanish_oak.n.01',\n",
       "   'parents': 'oak.n.02',\n",
       "   'grandparents': None,\n",
       "   'case': 'predict_hypernym'}],\n",
       " [{'children': 'singular_matrix.n.01',\n",
       "   'parents': 'square_matrix.n.01',\n",
       "   'grandparents': None,\n",
       "   'case': 'predict_hypernym'},\n",
       "  {'children': 'powdered_sugar.n.01',\n",
       "   'parents': 'granulated_sugar.n.01',\n",
       "   'grandparents': None,\n",
       "   'case': 'predict_hypernym'},\n",
       "  {'children': 'general_anesthesia.n.01',\n",
       "   'parents': 'anesthesia.n.01',\n",
       "   'grandparents': None,\n",
       "   'case': 'predict_hypernym'}])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train[:3], test[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_definitions(elem):\n",
    "    if elem['case'] == 'predict_hypernym':\n",
    "        elem['child_def'] = wn.synset(elem['children'].replace(' ', '_')).definition()\n",
    "      #  elem['parent_def'] = wn.synset(elem['parents']).definition()\n",
    "    elif elem['case'] == 'predict_multiple_hypernyms':\n",
    "        elem['child_def'] = wn.synset(elem['children'].replace(' ', '_')).definition()\n",
    "\n",
    "    elif elem['case'] == 'simple_triplet_grandparent':\n",
    "        elem['child_def'] = wn.synset(elem['children'].replace(' ', '_')).definition()\n",
    "        elem['grandparent_def'] = wn.synset(elem['grandparents']).definition()\n",
    "    elif elem['case'] == 'only_child_leaf':\n",
    "        elem['grandparent_def'] = wn.synset(elem['grandparents']).definition()\n",
    "        elem['parent_def'] = wn.synset(elem['parents']).definition()\n",
    "    elif elem['case'] == 'simple_triplet_2parent':\n",
    "        elem['1parent_def'] = wn.synset(elem['parents'][0]).definition()\n",
    "        elem['2parent_def'] = wn.synset(elem['parents'][1]).definition()\n",
    "    else:\n",
    "        elem['parent_def'] = wn.synset(elem['parents']).definition()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "for i, elem in enumerate(train):\n",
    "    try:\n",
    "        add_definitions(elem)\n",
    "    except:\n",
    "        print(i, elem)\n",
    "        train.remove(elem)\n",
    "\n",
    "counter = 0\n",
    "for i, elem in enumerate(test):\n",
    "    try:\n",
    "        add_definitions(elem)\n",
    "    except:\n",
    "        print(i, elem)\n",
    "        counter += 1\n",
    "        test.remove(elem)\n",
    "\n",
    "print(counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_out = '/home/LLM_Taxonomy/wnet/unified_wnet_noun_verb_def_train.pickle'\n",
    "test_out = '/home/LLM_Taxonomy/wnet/unified_wnet_noun_verb_def_test.pickle'\n",
    "\n",
    "with open(train_out, 'wb') as f:\n",
    "    pickle.dump(train, f)\n",
    "\n",
    "with open(test_out, 'wb') as f:\n",
    "    pickle.dump(test, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
